{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Titanic survival dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to fill #TODO places with your code and answer questions  \n",
    "\n",
    "Here are some features that can help you determine who survived in Titanic.  \n",
    "Our quality metric - area under the roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 1337\n",
    "# !!you should use that seed in all models which accepts random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1:\n",
    "# encode Sex column\n",
    "# hint: use LabelEncoder\n",
    "\n",
    "\n",
    "df['Sex'] = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print columns with missing values\n",
    "for name in df.columns:\n",
    "    print(name, np.sum(pd.isnull(df[name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with \"unkown\"\n",
    "df.loc[df.Cabin.isnull(), 'Cabin'] = 'unknown'\n",
    "\n",
    "# fill Fare with median value\n",
    "df.loc[ np.isnan(df['Fare']), 'Fare'] = df['Fare'].median()\n",
    "\n",
    "# TODO 2\n",
    "# Replace missing values in Embarked column with most common port\n",
    "# hint: use Series.mode method\n",
    "\n",
    "df.loc[ df.Embarked.isnull(), 'Embarked' ] = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3\n",
    "# train Random Forest regression model to fill missing values in Age column\n",
    "# use  100 estimators, set oob_score in True, other params by default\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "age_columns = ['Age', 'Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', \n",
    "       'Fare', 'Embarked']\n",
    "\n",
    "age_df = df[age_columns]\n",
    "\n",
    "age_df['Embarked'] = preprocessing.LabelEncoder().fit_transform(age_df['Embarked'])\n",
    "train_df = age_df.loc[age_df.Age.notnull()]\n",
    "test_df = age_df.loc[age_df.Age.isnull()]\n",
    "print('train age', train_df.shape[0])\n",
    "print('test age', test_df.shape[0])\n",
    "\n",
    "\n",
    "# df.loc[df.Age.isnull(), 'Age'] = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: what is the oob_score of trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4\n",
    "# Create a dataframe of dummy variables for each distinct value of Embarked\n",
    "# hint: use DataFrame.get_dummies method\n",
    "\n",
    "# Rename the columns from 'S', 'C', 'Q' to 'Embarked_S', 'Embarked_C', 'Embarked_Q'\n",
    "# hint: use pandas.rename method\n",
    "\n",
    "# Add the new dummy variables back to the original data set\n",
    "# hint: use pandas.concat method\n",
    "\n",
    "df = ?\n",
    "\n",
    "\n",
    "df['Embarked'] = preprocessing.LabelEncoder().fit_transform(df['Embarked'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5\n",
    "\n",
    "# create feature for the alphabetical part of the cabin number\n",
    "# hint: use re module\n",
    " \n",
    "# convert the distinct cabin letters with incremental integer values\n",
    "# hint: use pandas.factorize method\n",
    "\n",
    "df['CabinLetter'] =?\n",
    "\n",
    "df['Cabin'] = preprocessing.LabelEncoder().fit_transform(df['Cabin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6\n",
    "\n",
    "# Divide all fares into quartiles\n",
    "# hint: use pandas.qcut\n",
    "\n",
    "# create dummies from the result and add tham to dataset\n",
    "\n",
    "df = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Name', 'Ticket'], axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7\n",
    "# make dimension reduction with PCA, leave enough components to explain 98% variance of the data, \n",
    "# other params by default. Project dataset on these components\n",
    "# Of course, you should not project your target variable\n",
    "\n",
    "\n",
    "X_pca = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: how many components are in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 8\n",
    "# plot feature importances from Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df.Survived.values\n",
    "features_list = X.columns\n",
    "X = X.as_matrix()\n",
    " \n",
    "# Train Random Forest Classifier on datasert with 5000 trees, other params by default\n",
    "# get feature importances from model\n",
    "\n",
    "feature_importance = ?\n",
    " \n",
    "# make importances relative to max importance\n",
    "feature_importance = feature_importance / feature_importance.max()\n",
    " \n",
    "# A threshold below which to drop features from the final data set. Specifically, this number represents\n",
    "# the percentage of the most important feature's importance value\n",
    "fi_threshold = 0.10\n",
    " \n",
    "# Get the indexes of all features over the importance threshold\n",
    "\n",
    "important_idx = ?\n",
    " \n",
    "# Create a list of all the feature names above the importance threshold\n",
    "important_features = features_list[important_idx]\n",
    " \n",
    "# Get the sorted indexes of important features\n",
    "sorted_idx = np.argsort(feature_importance[important_idx])[::-1]\n",
    " \n",
    "\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[important_idx][sorted_idx[::-1]], align='center')\n",
    "plt.yticks(pos, important_features[sorted_idx[::-1]])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.draw()\n",
    "plt.show()\n",
    " \n",
    "# Remove non-important features from the feature set\n",
    "# X = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: What are 4 top most important features?##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack features\n",
    "\n",
    "X = np.hstack([X, X_pca])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 9 \n",
    "# plot learning curves for Random Forest Model (for train and test)\n",
    "# hint: look at sklearn.learning_curve module. Look at the docs\n",
    "# use Random Forest with 500 trees, 10-fold cross-validation for roc_auc scoring.\n",
    "# \n",
    "train_sizes = np.linspace(0.1, 1., 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 10\n",
    "# plot roc auc curve for Random Forest for test dataset\n",
    "# hint: use roc_curve and auc functions from sklearn.metrics\n",
    "# use Random Forest with 5000 trees\n",
    "# use test_size=0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: What is the roc auc score on test dataset ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
