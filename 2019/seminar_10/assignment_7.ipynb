{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7\n",
    "\n",
    "Train a Transformer model for Machine Translation from Russian to English.  \n",
    "Dataset: http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz   \n",
    "Make all source and target text to lower case.  \n",
    "Use following tokenization for english:  \n",
    "```\n",
    "import sentencepiece as spm\n",
    "\n",
    "...\n",
    "spm.SentencePieceTrainer.Train('--input=data/text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')\n",
    "\n",
    "tok_en = spm.SentencePieceProcessor()\n",
    "tok_en.load('bpe_en.model')\n",
    "\n",
    "TGT = data.Field(\n",
    "    fix_length=50,\n",
    "    init_token='<s>',\n",
    "    eos_token='</s>',\n",
    "    lower=True,\n",
    "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "...\n",
    "TGT.build_vocab(..., min_freq=5)\n",
    "...\n",
    "\n",
    "```\n",
    "Score: corpus-bleu `nltk.translate.bleu_score.corpus_bleu`  \n",
    "Use last 1000 sentences for model evalutation (test dataset).  \n",
    "Use your target sequence tokenization for BLEU score.  \n",
    "Use max_len=50 for sequence prediction.  \n",
    "\n",
    "\n",
    "Hint: You may consider much smaller model, than shown in the example.  \n",
    "\n",
    "Baselines:  \n",
    "[4 point] BLEU = 0.05  \n",
    "[6 point] BLEU = 0.10  \n",
    "[9 point] BLEU = 0.15  \n",
    "\n",
    "[1 point] Share weights between target embeddings and output dense layer. Notice, they have the same shape.\n",
    "\n",
    "\n",
    "Readings:\n",
    "1. BLUE score how to https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "1. Transformer code and comments http://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torchtext import datasets, data\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize english \n",
    "with open('data/news-commentary-v13.ru-en.en') as f:\n",
    "    with open('data/text.en', 'w') as out:\n",
    "            out.write(f.read().lower())\n",
    "        \n",
    "spm.SentencePieceTrainer.Train('--input=data/text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize russian\n",
    "\n",
    "<TODO>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ru = spm.SentencePieceProcessor()\n",
    "tok_ru.load('bpe_ru.model')\n",
    "\n",
    "tok_en = spm.SentencePieceProcessor()\n",
    "tok_en.load('bpe_en.model')\n",
    "\n",
    "SRC = data.Field(\n",
    "    fix_length=50,\n",
    "    init_token='<s>',\n",
    "    eos_token='</s>',\n",
    "    lower=True,\n",
    "    tokenize = lambda x: tok_ru.encode_as_pieces(x),\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "TGT = data.Field(\n",
    "    fix_length=50,\n",
    "    init_token='<s>',\n",
    "    eos_token='</s>',\n",
    "    lower=True,\n",
    "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "fields = (('src', SRC), ('tgt', TGT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9efabb9d3b84c1e8fd3599ed44eff7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/text.ru') as f:\n",
    "    src_snt = list(map(str.strip, f.readlines()))\n",
    "    \n",
    "with open('data/text.en') as f:\n",
    "    tgt_snt = list(map(str.strip, f.readlines()))\n",
    "    \n",
    "examples = [data.Example.fromlist(x, fields) for x in tqdm(zip(src_snt, tgt_snt))]\n",
    "test = data.Dataset(examples[-1000:], fields)\n",
    "train, valid = data.Dataset(examples[:-1000], fields).split(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: ▁в ▁частности , ▁сантос ▁подписал ▁в ▁июне ▁ 2011 ▁года ▁в ▁присутствии ▁генерального ▁секретаря ▁оон ▁пан ▁ги ▁муна ▁закон ▁о ▁возме щении ▁ущерба ▁жертв ▁и ▁восстановлении ▁земельных ▁влад ений .\n",
      "tgt: ▁most ▁notably , ▁santos ▁signed ▁the ▁victims ▁and ▁land ▁restitution ▁law ▁in ▁ j une ▁ 2011 , ▁in ▁the ▁presence ▁of ▁united ▁nations ▁secretary - general ▁ban ▁ki - moon .\n"
     ]
    }
   ],
   "source": [
    "print('src: ' + \" \".join(train.examples[100].src))\n",
    "print('tgt: ' + \" \".join(train.examples[100].tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210743, 23416, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TGT.build_vocab(train, min_freq=5)\n",
    "SRC.build_vocab(train, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import make_model, Batch\n",
    "\n",
    "    \n",
    "class BucketIteratorWrapper(DataLoader):\n",
    "    __initialized = False\n",
    "\n",
    "    def __init__(self, iterator: data.Iterator):\n",
    "#         super(BucketIteratorWrapper,self).__init__()\n",
    "        self.batch_size = iterator.batch_size\n",
    "        self.num_workers = 1\n",
    "        self.collate_fn = None\n",
    "        self.pin_memory = False\n",
    "        self.drop_last = False\n",
    "        self.timeout = 0\n",
    "        self.worker_init_fn = None\n",
    "        self.sampler = iterator\n",
    "        self.batch_sampler = iterator\n",
    "        self.__initialized = True\n",
    "\n",
    "    def __iter__(self):\n",
    "        return map(\n",
    "            lambda batch: Batch(batch.src, batch.tgt, pad=TGT.vocab.stoi['<pad>']),\n",
    "            self.batch_sampler.__iter__()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler)\n",
    "    \n",
    "class MyCriterion(nn.Module):\n",
    "    def __init__(self, pad_idx):\n",
    "        super(MyCriterion, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_idx)\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        x = x.contiguous().permute(0,2,1)\n",
    "        ntokens = (target != self.pad_idx).data.sum()\n",
    "        \n",
    "        return self.criterion(x, target) / ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "batch_size = <TODO>\n",
    "num_epochs = <TODO>\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train, valid, test), \n",
    "                                              batch_sizes=(batch_size, batch_size, batch_size), \n",
    "                                  sort_key=lambda x: len(x.src),\n",
    "                                  shuffle=True,\n",
    "                                  device=DEVICE,\n",
    "                                  sort_within_batch=False)\n",
    "                                  \n",
    "train_iter = BucketIteratorWrapper(train_iter)\n",
    "valid_iter = BucketIteratorWrapper(valid_iter)\n",
    "test_iter = BucketIteratorWrapper(test_iter)\n",
    "\n",
    "model = <TODO>\n",
    "model = model.to(DEVICE)\n",
    "criterion = <TODO>\n",
    "optimizer = <TODO>\n",
    "scheduler = <TODO>\n",
    "\n",
    "# share weights\n",
    "<TODO>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_iter, model, criterion):\n",
    "    total_loss = 0\n",
    "    data_iter = tqdm(data_iter)\n",
    "    counter = 0\n",
    "    for batch in data_iter:\n",
    "        \n",
    "        <TODO>\n",
    "        \n",
    "        total_loss += loss\n",
    "        data_iter.set_postfix(loss = loss)\n",
    "        counter +=1\n",
    "        \n",
    "    total_loss /= counter\n",
    "    return total_loss\n",
    "\n",
    "def valid_epoch(data_iter, model, criterion):\n",
    "    total_loss = 0\n",
    "    data_iter = tqdm(data_iter)\n",
    "    counter = 0\n",
    "    for batch in data_iter:\n",
    "        \n",
    "        <TODO>\n",
    "        \n",
    "        total_loss += loss\n",
    "        data_iter.set_postfix(loss = loss)\n",
    "        counter +=1\n",
    "        \n",
    "    total_loss /= counter\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss = train_epoch(train_iter, model, criterion)\n",
    "    print('train', loss)\n",
    "    wandb.log({'loss/train': loss}, step=epoch)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = valid_epoch(valid_iter, model, criterion)\n",
    "        scheduler.step(loss)\n",
    "        print('valid', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, src, src_mask, max_len=10, k=5):\n",
    "    <TODO>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source: рост\n",
      "Translation:\n",
      "pred -1.31: growth\n",
      "pred -2.03: growth growth\n",
      "pred -3.63: rising growth\n",
      "pred -3.89: growth in growth\n",
      "pred -4.38: growth growth growth\n",
      "Target: inflation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis.litvinov/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(valid_iter):\n",
    "        src = batch.src[:1]\n",
    "        src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
    "        beam = beam_search(model, src, src_key_padding_mask)\n",
    "        \n",
    "        seq = []\n",
    "        for i in range(1, src.size(1)):\n",
    "            sym = SRC.vocab.itos[src[0, i]]\n",
    "            if sym == \"</s>\": break\n",
    "            seq.append(sym)\n",
    "        seq = tok_ru.decode_pieces(seq)\n",
    "        print(\"\\nSource:\", seq)\n",
    "        \n",
    "        print(\"Translation:\")\n",
    "        for pred, pred_proba in beam:                \n",
    "            seq = []\n",
    "            for i in range(1, pred.size(1)):\n",
    "                sym = TGT.vocab.itos[pred[0, i]]\n",
    "                if sym == \"</s>\": break\n",
    "                seq.append(sym)\n",
    "            seq = tok_en.decode_pieces(seq)\n",
    "            print(f\"pred {pred_proba:.2f}:\", seq)\n",
    "                \n",
    "        seq = []\n",
    "        for i in range(1, batch.tgt.size(1)):\n",
    "            sym = TGT.vocab.itos[batch.tgt[0, i]]\n",
    "            if sym == \"</s>\": break\n",
    "            seq.append(sym)\n",
    "        seq = tok_en.decode_pieces(seq)\n",
    "        print(\"Target:\", seq)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "references = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        <TODO>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22829332685417014"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu(references, hypotheses, \n",
    "            smoothing_function=translate.bleu_score.SmoothingFunction().method3,\n",
    "            auto_reweigh=True\n",
    "           )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
