{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "## Outline:  \n",
    "1. Ranking \n",
    "1. Siamise Networks\n",
    "1. Triplet Loss\n",
    "1. KNN\n",
    "1. LSH\n",
    "\n",
    "## Readings\n",
    "1. MAP https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52\n",
    "1. Hierarchial Navigable Small Worlds https://arxiv.org/abs/1603.09320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Ranking \n",
    "\n",
    "Given a query, return a list of documents from database, sorted in descending order by their relevance. \n",
    "\n",
    "Pointwise approach: *relevance = h(query, document)*, $relevance \\in R$  \n",
    "Pairwise approach: *relevance = h(query, document)*, $relevance \\in \\{0,1\\}$  \n",
    "\n",
    "<img src=images/if.png style=\"height:300px\">\n",
    "\n",
    "\n",
    "Evalulation:\n",
    "For a given query predict list of documents of a finite size and look how well they are sorted by the true relevance.  \n",
    "\n",
    "**Mean Average Precision**\n",
    "\n",
    "Supports only binary relevance.  \n",
    "\n",
    "$$ AP_n =  \\frac 1  {GTP} \\sum_{i=1}^n \\frac {TP_i} i$$\n",
    "$$ MAP_n = \\frac 1 Q \\sum_{q=1}^Q AP_n$$\n",
    "\n",
    "where  \n",
    "$GTP$ - total number of ground true positives   \n",
    "$TP_i$ - number of true positives up to i-th position  \n",
    "$Q$ - number of queries   \n",
    "\n",
    "**Normalized Dicounted Commulative Gain**\n",
    "\n",
    "Supports multilevel relevance.  \n",
    "\n",
    "$$ DCG_n = \\sum_{i=1}^n \\frac {rel_i} {\\log_2 (i+1)} $$\n",
    "$$ NDCG_n = \\frac {DCG_n} {IDCG_n}$$\n",
    "\n",
    "where  \n",
    "$DCG$ - dicounted commulative gain  \n",
    "$IDCG$ - ideal discounted cumulative gain (as if your recommendation was as good as possible)  \n",
    "$rel_i$ - relevance score for i-th position in predicted ranked list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2 Siamese Networks\n",
    "\n",
    "<img src=images/siam1.jpeg style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199530</td>\n",
       "      <td>301049</td>\n",
       "      <td>301050</td>\n",
       "      <td>As a Canadian student, is it wiser to complete...</td>\n",
       "      <td>How much will it cost to Indian student to stu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>387099</td>\n",
       "      <td>29541</td>\n",
       "      <td>519407</td>\n",
       "      <td>What is your favorite Indian sweet dish?</td>\n",
       "      <td>What's your favorite Indian dish? Why?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>337316</td>\n",
       "      <td>464776</td>\n",
       "      <td>464777</td>\n",
       "      <td>Is there proof of Jon being Rhaegar and Lyanna...</td>\n",
       "      <td>Where does GRRM imply that Jon Snow is Rhaegar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164415</td>\n",
       "      <td>255489</td>\n",
       "      <td>255490</td>\n",
       "      <td>Knowing how Prithviraj's last 3 films were flo...</td>\n",
       "      <td>Which is the Best Comedy scene in Malayalam ci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>382707</td>\n",
       "      <td>514592</td>\n",
       "      <td>514593</td>\n",
       "      <td>What causes damage to the somatosensory cortex...</td>\n",
       "      <td>What causes damage to the somatosensory cortex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  199530  301049  301050  As a Canadian student, is it wiser to complete...   \n",
       "1  387099   29541  519407           What is your favorite Indian sweet dish?   \n",
       "2  337316  464776  464777  Is there proof of Jon being Rhaegar and Lyanna...   \n",
       "3  164415  255489  255490  Knowing how Prithviraj's last 3 films were flo...   \n",
       "4  382707  514592  514593  What causes damage to the somatosensory cortex...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  How much will it cost to Indian student to stu...             0  \n",
       "1             What's your favorite Indian dish? Why?             0  \n",
       "2  Where does GRRM imply that Jon Snow is Rhaegar...             0  \n",
       "3  Which is the Best Comedy scene in Malayalam ci...             0  \n",
       "4  What causes damage to the somatosensory cortex...             1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from Quora duplicate detection\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2019 12:48:59 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "# Compute two different representation for each token.\n",
    "# Each representation is a linear weighted combination for the\n",
    "# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "xq1_train = batch_to_ids(df_train.question1.values)\n",
    "xq2_train = batch_to_ids(df_train.question2.values)\n",
    "y_train = tt.from_numpy(df_train.is_duplicate.values).float()\n",
    "\n",
    "xq1_val = batch_to_ids(df_val.question1.values)\n",
    "xq2_val = batch_to_ids(df_val.question2.values)\n",
    "y_val = tt.from_numpy(df_val.is_duplicate.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(xq1_train, xq2_train, y_train), batch_size=batch_size)\n",
    "val_loader = DataLoader(TensorDataset(xq1_val, xq2_val, y_val), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(model, iterator, optimizer, curr_epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = model(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        \n",
    "        loss_smoothing = i / (i+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "\n",
    "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "def _test_epoch(model, iterator):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    with tt.no_grad():\n",
    "        for batch in iterator:\n",
    "            loss = model(batch)\n",
    "            epoch_loss += loss.data.item()\n",
    "\n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "\n",
    "def nn_train(model, train_iterator, valid_iterator, optimizer, n_epochs=100,\n",
    "          scheduler=None, early_stopping=0):\n",
    "\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    best_epoch = None\n",
    "    history = pd.DataFrame()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = _train_epoch(model, train_iterator, optimizer, epoch)\n",
    "        valid_loss = _test_epoch(model, valid_iterator)\n",
    "\n",
    "        valid_loss = valid_loss\n",
    "        print('validation loss %.5f' % valid_loss)\n",
    "\n",
    "        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n",
    "        history = history.append(record, ignore_index=True)\n",
    "\n",
    "        if early_stopping > 0:\n",
    "            if valid_loss > prev_loss:\n",
    "                es_epochs += 1\n",
    "            else:\n",
    "                es_epochs = 0\n",
    "\n",
    "            if es_epochs >= early_stopping:\n",
    "                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n",
    "                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n",
    "                break\n",
    "\n",
    "            prev_loss = min(prev_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, elmo, criterion):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.elmo = elmo\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.fc = nn.Linear(1024*2, 128)\n",
    "        \n",
    "        self.out = nn.Linear(128*3, 1)\n",
    "        \n",
    "    def branch(self, x):\n",
    "        x = self.elmo(x)['elmo_representations']\n",
    "        x = tt.cat(x, dim=-1)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        q1, q2, y = batch\n",
    "        \n",
    "        q1 = self.branch(q1)\n",
    "        q2 = self.branch(q2)\n",
    "        \n",
    "        # simetric functions\n",
    "        x = tt.cat([tt.abs(q1-q2), q1*q2, q1+q2], dim=-1)\n",
    "        \n",
    "        x = self.out(x).squeeze(1)\n",
    "        loss = self.criterion(x,y)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "model = MyModel(elmo, nn.BCEWithLogitsLoss())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "nn_train(model, train_loader, val_loader, optimizer, n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Triplet loss\n",
    "\n",
    "Distance to samples from the same class should be less than to samples from other classes\n",
    "\n",
    "Euclidean distance: \n",
    "<img src=images/triplet.png height=200/>\n",
    "\n",
    "Cosine distance: \n",
    "<img src=images/triplet2.png height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(anchor_embed, pos_embed, neg_embed):\n",
    "    return F.cosine_similarity(anchor_embed, neg_embed) - F.cosine_similarity(anchor_embed, pos_embed)\n",
    "    \n",
    "    \n",
    "class Tripletnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tripletnet, self).__init__()\n",
    "        ...\n",
    "        \n",
    "    def branch(self, x):\n",
    "        ....\n",
    "\n",
    "    def forward(self, anchor, pos, neg):\n",
    "        \n",
    "        anchor = self.branch(anchor)\n",
    "        pos = self.branch(pos)\n",
    "        neg = self.branch(neg)\n",
    "        \n",
    "        return triplet_loss(anchor, pos, neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Negative Mining\n",
    "\n",
    "\n",
    "Sometimes, if you use random samples as negative examples, classification may be too easy for you model.  \n",
    "You can consider taking samples from previous epoch, where your model made mistakes, as negative examples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another loss\n",
    "\n",
    "Generalization of triplet loss\n",
    "\n",
    "$$ Loss(D) = - \\frac 1 B \\sum_{i=1}^B \\log \\frac {\\exp (D_{ii})} {\\sum_{j=1}^B \\exp (D_{ij})} $$\n",
    "\n",
    "where  \n",
    "$D$ - some similarity matrix between samples in a batch  \n",
    "$B$ - batch size  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 K-Nearest Neighbors (KNN)\n",
    "\n",
    "Training complexity: O(1) - just remember all train set  \n",
    "Inference complexity: O(n) - have to compare each test sample with all train samples  \n",
    "\n",
    "<img src=images/knn.png style=\"height:300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Locale Sensitive Hashing (LSH)\n",
    "\n",
    "Good implementation can be found here `https://github.com/spotify/annoy`\n",
    "\n",
    "Definition:  \n",
    "LSH family $F$ is a family of hash functions that maps metric space $M$ to set of buckets $S$.  \n",
    "$$ h: M \\rightarrow S $$\n",
    "\n",
    "Let  \n",
    "$p,q \\in M $ - points in space  \n",
    "$d$ be the metric in $M$  \n",
    "$c$ - some scalar, $c > 1$\n",
    ", then for $h \\in F$:  \n",
    "\n",
    "* if $d(p,q) \\leq R$ then $P[h(p) = h(q)] \\geq p_1$  \n",
    "* if $d(p,q) \\geq cR$ then $P[h(p) = h(q)] \\leq p_2$  \n",
    "\n",
    "And family $F$ is called $(R, cR, p_1, p_2)$ - sensitive\n",
    "\n",
    "\n",
    "Assumption: uniform distribution\n",
    "    \n",
    "<img src=images/lsh1.jpeg height=400/>\n",
    "\n",
    "Amplification:  \n",
    "\n",
    "1. AND construction\n",
    "\n",
    "Define new family of hash functions $G = {g}$, where each consists of k hash functions from $F$ chosen at random $g = h_1, ..., h_k$.\n",
    "\n",
    "$g(p) = g(q)$ iff $h_i(p) = h_i(q)$ **for all** $i$  \n",
    "\n",
    "Then, family $G$ is $(d_1, d_2, p_1^k, p_2^k)$ - sensitive\n",
    "\n",
    "2. OR construction\n",
    "\n",
    "Define new family of hash functions $G = {g}$, where each consists of k hash functions from $F$ chosen at random $g = h_1, ..., h_k$.\n",
    "\n",
    "$g(p) = g(q)$ iff $h_i(p) = h_i(q)$ **at least for one** $i$  \n",
    "\n",
    "Then, family $G$ is $(d_1, d_2, 1 - (1 - p_1)^k, 1 - (1- p_2)^k)$ - sensitive\n",
    "\n",
    "LSH maps:  \n",
    "<img src=images/lsh2.png style=\"height:400px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
