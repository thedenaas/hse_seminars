{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Recurrent Neural Networks\n",
    "\n",
    "\n",
    "## 4.1 Vanilla RNN\n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"height:300px\">\n",
    "\n",
    "In general can be described by 2 equations:  \n",
    "$$ h^{(t)} = f(h^{(t-1)}, x^{(t)}; \\theta)$$\n",
    "$$ o^{(t)} = g(h^{(t-1)}, \\theta)$$\n",
    "\n",
    "where $x^{(t)}$ - input at time $t$,  \n",
    "$h^{(t)}$ - hidden state at time $t$,  \n",
    "$o^{(t)}$ - output at time $t$,  \n",
    "$f,g$ - some functions, parametrized by learnable weights $\\theta$  \n",
    "\n",
    "For Vanilla RNN usually:\n",
    "$$h^{(t)} = tanh( b + W_{hh} h^{(t-1)}  + W_{xh} x^{(t)})$$\n",
    "$$o^{(t)} = softmax( c + W_{ho} h^{(t)})$$\n",
    "\n",
    "If not specified, $h^{(0)}$ is initialized with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Training RNN\n",
    "\n",
    "Backpropagation Through Time (BPTT)\n",
    "\n",
    "Because every next step of RNN becomes dependent on the previous step, it means that gradient estimation becomes dependent on the length of the sequence. Weights of RNN are shared across steps and gradients from every time step will be summed.\n",
    "\n",
    "For example, for $T=3$:\n",
    "$$\\frac {\\partial L} {\\partial W_{hh}} = \\frac {\\partial L} {\\partial h^{(3)}} \\frac {\\partial h^{(3)}} {\\partial W_{hh}} + \\frac {\\partial L} {\\partial h^{(2)}} \\frac {\\partial h^{(2)}} {\\partial W_{hh}} + \\frac {\\partial L} {\\partial h^{(1)}} \\frac {\\partial h^{(1)}} {\\partial W_{hh}}$$\n",
    "\n",
    "To make gradient estimation more managable, in BPTT we introduce a sliding window, where we compute gradients and make weight updates.\n",
    "\n",
    "<img src=\"images/bptt1.png\" style=\"height:300px\">\n",
    "\n",
    "<img src=\"images/bptt2.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Problems with Vanilla RNN\n",
    "\n",
    "Vanilla RNNs are subject to exploding and vanishing gradients, which makes training unstable.\n",
    "\n",
    "Look closely on \n",
    "$$h^{(t)} = tanh( b + W_{hh} h^{(t-1)}  + W_{xh} x^{(t)})$$\n",
    "\n",
    "Let's try to compute $\\frac {\\partial L} {\\partial h}$.  \n",
    "For the sake of simplicity, lets change activation function $tanh \\rightarrow 1$, which make sense, if we make the approximationa around $0$, where $tanh$ has linear behavior. Also, suppose that only the last time step is used for loss computation, so $L$ only depends on $h^{(N)}$.\n",
    "\n",
    "So, \n",
    "$$h^{(t)} = b + W_{hh} h^{(t-1)}  + W_{xh} x^{(t)}$$\n",
    "\n",
    "$$\\frac {\\partial L} {\\partial h} = \\frac {\\partial L} {\\partial h^{(N)}} ( W_{hh} \\frac {\\partial h^{(N)}} {\\partial h^{(N-1)}} )  ( W_{hh} \\frac {\\partial h^{(N-1)}} {\\partial h^{(N-2)}} ) ... ( W_{hh} \\frac {\\partial h^{(1)}} {\\partial h^{(0)}} ) = \\frac {\\partial L} {\\partial h^{(N)}} W_{hh}^T $$\n",
    "\n",
    "Because $W_{hh}^T$ is a power of square matrix, we can calculate it by eigendecomposition: \n",
    "\n",
    "$$W_{hh}^N  = U \\Lambda^N U^T$$, \n",
    "where once again, $U$ - unitary matrix, and $\\Lambda$ - diagonal matrix\n",
    "\n",
    "Now, \n",
    "$$\\frac {\\partial L} {\\partial h} =  \\frac {\\partial L} {\\partial h^{(N)}} U \\Lambda^N U^T $$\n",
    "\n",
    "Our gradient becomes highly dependent on the conditional number of $cond(W_{hh}) = \\frac {\\max(\\lambda_ii)} {\\min(\\lambda_ii)}$ (relation between maximumal and minimal eigenvalue of $W_{hh}$).\n",
    "\n",
    "For example, if we have $cond(W_{hh}) > 1$, then we get exploding gradientds. Because \n",
    "$$\\lim_{N \\rightarrow \\infty} {1.001}^N = \\infty$$\n",
    "\n",
    "<img src=\"images/exploding.png\" style=\"height:300px\">\n",
    "\n",
    "Problem with exploding gradient is that we can move too far from our optimal solution. \n",
    "Exploding gradients can be managed by **gradient clipping**.\n",
    "See `torch.nn.utils.clip_grad`\n",
    "\n",
    "If we have $cond(W_{hh}) < 1$, then we have vanishing gradients. Because \n",
    "$$\\lim_{N \\rightarrow \\infty} {0.999}^N = 0$$\n",
    "Problem with vanishing gradient is that we can think, that the optimal solution is found. Remember, that in a local minimum gradient is also zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the problem of vanishing gradients the following architectures were developed. (There are more of them, but theese are the most commonly used)\n",
    "\n",
    "## 4.2 Long Short-Term Memory (LSTM)\n",
    "\n",
    "<img src=\"images/lstm.png\" style=\"height:300px\">\n",
    "\n",
    "\n",
    "<img src=\"images/lstm2.png\" style=\"height:300px\">\n",
    "\n",
    "### Description\n",
    "\n",
    "<img src=\"images/lstm_desc.svg\" style=\"height:300px\">\n",
    "\n",
    "$f_t$ - forget gate,  \n",
    "$i_t$ - input gate,  \n",
    "$o_t$ - output gate,  \n",
    "$c_t$ - cell memory,  \n",
    "$h_t$ - hidden state\n",
    "\n",
    "Compared to Vanilla RNN, cell state in LSTM allows \"free\" gradient propagation without reccurent relations.\n",
    "\n",
    "There are **4 times more parameters** in LSTM cell, then in RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.3 GRU RNN\n",
    "\n",
    "<img src=\"images/gru.png\" style=\"height:300px\">\n",
    "\n",
    "### Description\n",
    "\n",
    "<img src=\"images/gru_desc.png\" style=\"height:300px\">\n",
    "\n",
    "$r_t$ - record gate,  \n",
    "$z_t$ - forget gate,    \n",
    "$h_t$ - hidden state\n",
    "\n",
    "* Combines input gate and forget gate\n",
    "* merges cell state and hidden state\n",
    "\n",
    "There are **3 times more parameters** in GRU cell, then in RNN cell.\n",
    "\n",
    "## 4.4 Bidirectional RNN\n",
    "\n",
    "<img src=\"images/bi.png\" style=\"height:300px\">\n",
    "\n",
    "`torch.nn.LSTM(.., bidirectional=True, ...)`\n",
    "\n",
    "\n",
    "Stacking RNNs:\n",
    "\n",
    "<img src=\"images/stack.jpeg\" style=\"height:300px\">\n",
    "\n",
    "`torch.nn.LSTM(.., num_layers=2, ...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "* There are 2 implementations of LSTM in pytorch: `torch.nn.LSTM` and `torch.nn.LSTMCell`.\n",
    "`torch.nn.LSTM` is CuDNN optimized version, where all training and BPTT are handled by the library. It also allows you to make bidirectional, stack layers,  or add dropout between layers. But you won't have access to intermidiate hidden and output states.\n",
    "\n",
    "`torch.nn.LSTMCell` is a basic version of LSTM, all training and BPTT must be managed by you.\n",
    "Same holds for GRU.\n",
    "\n",
    "* Though, in theory LSTM(GRU) can manage very long sequences, in practice it is not so good. Direction of RNN matters, in general last time steps will have more influence on the result. Consider Bidirectional if possible.\n",
    "\n",
    "* By default, sequence padding is handled by you. The library will not recognize padded symbols, and will process them as if they are part of the data. It may affect yout model perfomance and quality.\n",
    "Consider `torch.nn.utils.rnn.pack_padded_sequence` to specify length of each sequence.\n",
    "\n",
    "* Usually, RNN is more computationally expensive then CNN, because it cannot be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import Field, LabelField, BucketIterator, ReversibleField\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id,airline_sentiment,airline,retweet_count,text\r\n",
      "570306133677760513,neutral,Virgin America,0,@VirginAmerica What @dhepburn said.\r\n",
      "570301130888122368,positive,Virgin America,0,@VirginAmerica plus you've added commercials to the experience... tacky.\r\n",
      "570301083672813571,neutral,Virgin America,0,@VirginAmerica I didn't today... Must mean I need to take another trip!\r\n",
      "570301031407624196,negative,Virgin America,0,\"@VirginAmerica it's really aggressive to blast obnoxious \"\"entertainment\"\" in your guests' faces &amp; they have little recourse\"\r\n",
      "570300817074462722,negative,Virgin America,0,@VirginAmerica and it's a really big bad thing about it\r\n",
      "570300767074181121,negative,Virgin America,0,\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\r\n",
      "it's really the only bad thing about flying VA\"\r\n",
      "570300616901320704,positive,Virgin America,0,\"@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)\"\r\n",
      "570300248553349120,neutral,Virgin America,0,\"@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP\"\r\n"
     ]
    }
   ],
   "source": [
    "!head Tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_en.remove_pipe('tagger')\n",
    "spacy_en.remove_pipe('ner')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.lemma_ for tok in spacy_en.tokenizer(text) if tok.text.isalpha()]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes={\n",
    "    'negative':0,\n",
    "    'neutral':1,\n",
    "    'positive':2\n",
    "}\n",
    "\n",
    "TEXT = Field(include_lengths=True, batch_first=True, \n",
    "             tokenize=tokenizer,\n",
    "             eos_token='<eos>',\n",
    "             lower=True,\n",
    "             stop_words=nltk.corpus.stopwords.words('english')\n",
    "            )\n",
    "LABEL = LabelField(dtype=tt.int64, use_vocab=True, preprocessing=lambda x: classes[x])\n",
    "\n",
    "dataset = TabularDataset('Tweets.csv', format='csv', \n",
    "                         fields=[(None, None),('label', LABEL), (None, None),(None, None),('text', TEXT)], \n",
    "                         skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2329"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEXT.build_vocab(dataset, min_freq=10, vectors=\"glove.6B.100d\")\n",
    "TEXT.build_vocab(dataset, min_freq=5)\n",
    "len(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<eos>',\n",
       " 'flight',\n",
       " '-pron-',\n",
       " 'get',\n",
       " 'thank',\n",
       " 'hour',\n",
       " 'cancelled',\n",
       " 'service']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dataset.split(0.7, stratified=True)\n",
    "train, valid = train.split(0.7, stratified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([4498, 1518, 1158]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([x.label for x in train.examples], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([1927,  651,  496]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([x.label for x in valid.examples], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([2753,  930,  709]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([x.label for x in test.examples], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=embed_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           bidirectional=True,\n",
    "                           batch_first=True,\n",
    "                          )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size * 2 *2, 3)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        x, x_lengths = batch.text\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if x_lengths is not None:\n",
    "            x_lengths = x_lengths.view(-1).tolist()\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True)\n",
    "            \n",
    "        _, (hidden, cell) = self.rnn(x)\n",
    "        \n",
    "        hidden = hidden.transpose(0,1)\n",
    "        cell = cell.transpose(0,1)\n",
    "        hidden = hidden.contiguous().view(hidden.size(0),-1)\n",
    "        cell = cell.contiguous().view(cell.size(0),-1)\n",
    "        x = tt.cat([hidden, cell], dim=1).squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt.cuda.empty_cache()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model = MyModel(len(TEXT.vocab.itos),\n",
    "                embed_size=100,\n",
    "                hidden_size=128,\n",
    "               )\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(batch_size, batch_size, batch_size),\n",
    "    shuffle=True,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(model, iterator, optimizer, criterion, curr_epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        \n",
    "        loss_smoothing = i / (i+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "\n",
    "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "def _test_epoch(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    with tt.no_grad():\n",
    "        for batch in iterator:\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch.label)\n",
    "            epoch_loss += loss.data.item()\n",
    "\n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "\n",
    "def nn_train(model, train_iterator, valid_iterator, criterion, optimizer, n_epochs=100,\n",
    "          scheduler=None, early_stopping=0):\n",
    "\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    best_epoch = None\n",
    "    history = pd.DataFrame()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = _train_epoch(model, train_iterator, optimizer, criterion, epoch)\n",
    "        valid_loss = _test_epoch(model, valid_iterator, criterion)\n",
    "\n",
    "        valid_loss = valid_loss\n",
    "        print('validation loss %.5f' % valid_loss)\n",
    "\n",
    "        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n",
    "        history = history.append(record, ignore_index=True)\n",
    "\n",
    "        if early_stopping > 0:\n",
    "            if valid_loss > prev_loss:\n",
    "                es_epochs += 1\n",
    "            else:\n",
    "                es_epochs = 0\n",
    "\n",
    "            if es_epochs >= early_stopping:\n",
    "                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n",
    "                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n",
    "                break\n",
    "\n",
    "            prev_loss = min(prev_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b013b6259641da9e6e49bf5a8399d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 0', max=225, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.66278\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf23d0e22a14b87952f3cc57dfdd124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=225, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.64014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f9e8ad9b944b3e9cfe12ef040c3e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=225, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.65757\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464e63b80e9945839f6e19276a341d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=225, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.75362\n",
      "Early stopping! best epoch: 1 val 0.64014\n"
     ]
    }
   ],
   "source": [
    "nn_train(model, train_iterator, valid_iterator, criterion, optimizer, scheduler=scheduler, \n",
    "        n_epochs=10, early_stopping=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for autoregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "\n",
    "TEXT = ReversibleField(use_vocab=True, \n",
    "             include_lengths=True, \n",
    "             batch_first=True,\n",
    "             init_token='<start>', eos_token='<end>',\n",
    "             lower=True,\n",
    "            )\n",
    "\n",
    "TAG = ReversibleField(use_vocab=True, \n",
    "             include_lengths=False, \n",
    "             batch_first=True,\n",
    "             init_token='<start>', eos_token='<end>',\n",
    "            )\n",
    "\n",
    "\n",
    "train, valid, test = datasets.CoNLL2000Chunking.splits([('text', TEXT), ('label', TAG)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, valid, test, min_freq=5)\n",
    "TAG.build_vocab(train, valid, test, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' UNK ', '<pad>', '<start>', '<end>', ',', 'the', '.', 'of', 'to', 'a']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' UNK ', '<pad>', '<start>', '<end>', 'NN', 'IN', 'NNP', 'DT', 'NNS', 'JJ']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAG.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, target_vocab_size, embed_size, hidden_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=embed_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           bidirectional=True,\n",
    "                           batch_first=True,\n",
    "                          )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size * 2, target_vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        nn.init.uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        x, x_lengths = batch.text\n",
    "        batch_size = x.size(0)\n",
    "        total_length = x.size(1)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if x_lengths is not None:\n",
    "            x_lengths = x_lengths.view(-1).tolist()\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True)\n",
    "            \n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, total_length=total_length, batch_first=True)\n",
    "        \n",
    "        x = x.contiguous().view(batch_size * total_length, -1)\n",
    "        x = self.fc(x)\n",
    "        x = x.contiguous().view(batch_size , total_length, -1)\n",
    "        return x.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt.cuda.empty_cache()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model = MyModel(vocab_size=len(TEXT.vocab.itos),\n",
    "                target_vocab_size=len(TAG.vocab.itos),\n",
    "                embed_size=100,\n",
    "                hidden_size=128,\n",
    "               )\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(batch_size, batch_size, batch_size),\n",
    "    shuffle=True,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "\n",
    "# padding does not count into loss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d83761ca9dd4e2a9ee9cc696dfeeb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 0', max=252, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.60068\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80cc7e571f734d5aaf4bad4e53384315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=252, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.29925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac653642e14241428361cc9871e18572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=252, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.23225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa5a22316174d1a8b9001b4548298cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=252, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.20152\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b40dc7449f5411aa1f74ccded574e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 4', max=252, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.19936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336735570d0b4a5f858c866fd3773c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 5', max=252, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.18026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b0293eb58f4db3871eb3524e939ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 6', max=252, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.17999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deef2b80c1854624b852845ed4667c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 7', max=252, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.17590\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5368b13e536949b9b4cd1f68db81aeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 8', max=252, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.17829\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdb2ebbcb35476a90dac9d021db4d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 9', max=252, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.17607\n",
      "Early stopping! best epoch: 7 val 0.17590\n"
     ]
    }
   ],
   "source": [
    "nn_train(model, train_iterator, valid_iterator, criterion, optimizer, scheduler=scheduler, \n",
    "        n_epochs=10, early_stopping=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirty hack\n",
    "def reverse(self, batch):\n",
    "#     if self.use_revtok:\n",
    "#         try:\n",
    "#             import revtok\n",
    "#         except ImportError:\n",
    "#             print(\"Please install revtok.\")\n",
    "#             raise\n",
    "    if not self.batch_first:\n",
    "        batch = batch.t()\n",
    "    with tt.cuda.device_of(batch):\n",
    "        batch = batch.tolist()\n",
    "    batch = [[self.vocab.itos[ind] for ind in ex] for ex in batch]  # denumericalize\n",
    "\n",
    "    def trim(s, t):\n",
    "        sentence = []\n",
    "        for w in s:\n",
    "            if w == t:\n",
    "                break\n",
    "            sentence.append(w)\n",
    "        return sentence\n",
    "\n",
    "    batch = [trim(ex, self.eos_token) for ex in batch]  # trim past frst eos\n",
    "\n",
    "    def filter_special(tok):\n",
    "        return tok not in (self.init_token, self.pad_token)\n",
    "\n",
    "    batch = [filter(filter_special, ex) for ex in batch]\n",
    "#     if self.use_revtok:\n",
    "#         return [revtok.detokenize(ex) for ex in batch]\n",
    "    return [' '.join(ex) for ex in batch]\n",
    "\n",
    "TAG.reverse = reverse\n",
    "TEXT.reverse = reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "text:  corporate , other issues\n",
      "pred tags:  JJ , JJ NNS\n",
      "true tags:  JJ , JJ NNS\n",
      "\n",
      "1\n",
      "text:  short-term rates increased .\n",
      "pred tags:  JJ NNS VBN .\n",
      "true tags:  JJ NNS VBN .\n",
      "\n",
      "2\n",
      "text:  oct.  UNK  1989 :\n",
      "pred tags:  NNP NNP CD :\n",
      "true tags:  NNP CD CD :\n",
      "\n",
      "3\n",
      "text:  --  UNK  brown .\n",
      "pred tags:  : NNP NNP .\n",
      "true tags:  : NNP NNP .\n",
      "\n",
      "4\n",
      "text:  --  UNK   UNK  .\n",
      "pred tags:  : NNP NNP .\n",
      "true tags:  : NNP NNP .\n",
      "\n",
      "5\n",
      "text:  brown 's story :\n",
      "pred tags:  NNP POS NN :\n",
      "true tags:  NNP POS NN :\n",
      "\n",
      "6\n",
      "text:  he was  UNK  .\n",
      "pred tags:  PRP VBD JJ .\n",
      "true tags:  PRP VBD JJ .\n",
      "\n",
      "7\n",
      "text:  merck & co .\n",
      "pred tags:  NNP CC NNP .\n",
      "true tags:  NNP CC NNP .\n",
      "\n",
      "8\n",
      "text:  small talk :\n",
      "pred tags:  JJ NN :\n",
      "true tags:  NNP NNP :\n",
      "\n",
      "9\n",
      "text:   UNK   UNK  :\n",
      "pred tags:  NNP NNP :\n",
      "true tags:  NNP NNPS :\n",
      "\n",
      "10\n",
      "text:  clearly not .\n",
      "pred tags:  RB RB .\n",
      "true tags:  RB RB .\n",
      "\n",
      "11\n",
      "text:  warner-lambert co .\n",
      "pred tags:  NNP NNP .\n",
      "true tags:  NNP NNP .\n",
      "\n",
      "12\n",
      "text:  markets --\n",
      "pred tags:  NNS :\n",
      "true tags:  NNS :\n",
      "\n",
      "13\n",
      "text:  newspapers :\n",
      "pred tags:  NNS :\n",
      "true tags:  NNS :\n",
      "\n",
      "14\n",
      "text:   UNK  :\n",
      "pred tags:  NNP :\n",
      "true tags:  NN :\n",
      "\n",
      "15\n",
      "text:  energy :\n",
      "pred tags:  NNP :\n",
      "true tags:  NN :\n",
      "\n",
      "16\n",
      "text:  foreign bonds\n",
      "pred tags:  JJ NNS\n",
      "true tags:  JJ NNS\n",
      "\n",
      "17\n",
      "text:  mortgage-backed securities\n",
      "pred tags:  JJ NNPS\n",
      "true tags:  JJ NNPS\n",
      "\n",
      "18\n",
      "text:  treasury securities\n",
      "pred tags:  NNP NNPS\n",
      "true tags:  NNP NNPS\n",
      "\n",
      "19\n",
      "text:   UNK  financial\n",
      "pred tags:  NNP NNP\n",
      "true tags:  NNP NNP\n",
      "\n",
      "20\n",
      "text:  manufacturers hanover\n",
      "pred tags:  NNP NNP\n",
      "true tags:  NNP NNP\n",
      "\n",
      "21\n",
      "text:  wells fargo\n",
      "pred tags:  NNP NNP\n",
      "true tags:  NNP NNP\n",
      "\n",
      "22\n",
      "text:   UNK  out\n",
      "pred tags:  NNP IN\n",
      "true tags:  NNP IN\n",
      "\n",
      "23\n",
      "text:   UNK   UNK \n",
      "pred tags:  NNP NNP\n",
      "true tags:  NNP NNP\n",
      "\n",
      "24\n",
      "text:   UNK  :\n",
      "pred tags:  NNP :\n",
      "true tags:  NNS :\n",
      "\n",
      "25\n",
      "text:  black  UNK \n",
      "pred tags:  NNP NNP\n",
      "true tags:  NNP NNPS\n",
      "\n",
      "26\n",
      "text:  black males\n",
      "pred tags:  NNP NNPS\n",
      "true tags:  NNP NNPS\n",
      "\n",
      "27\n",
      "text:  white  UNK \n",
      "pred tags:  NNP NNP\n",
      "true tags:  NNP NNPS\n",
      "\n",
      "28\n",
      "text:  white males\n",
      "pred tags:  NNP NNPS\n",
      "true tags:  NNP NNPS\n",
      "\n",
      "29\n",
      "text:  municipals\n",
      "pred tags:  NNS\n",
      "true tags:  NNS\n",
      "\n",
      "30\n",
      "text:  citicorp\n",
      "pred tags:  NNP\n",
      "true tags:  NNP\n",
      "\n",
      "31\n",
      "text:   UNK \n",
      "pred tags:  NNP\n",
      "true tags:  NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in test_iterator:\n",
    "    pred = model(batch)\n",
    "    pred = tt.softmax(pred, dim=1)\n",
    "    pred = tt.argmax(pred, dim=1)\n",
    "    pred_tags = TAG.reverse(TAG, pred)\n",
    "    true_tags = TAG.reverse(TAG, batch.label)\n",
    "    true_text = TEXT.reverse(TEXT, batch.text[0])\n",
    "    \n",
    "    for i in range(len(pred_tags)):\n",
    "        print(i)\n",
    "        print('text: ', true_text[i])\n",
    "        print('pred tags: ', pred_tags[i])\n",
    "        print('true tags: ', true_tags[i])\n",
    "        print()\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 27, 33, 33])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' UNK ',\n",
       " '<pad>',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'JJ',\n",
       " ',',\n",
       " '.',\n",
       " 'CD',\n",
       " 'VBD',\n",
       " 'RB',\n",
       " 'VB',\n",
       " 'CC',\n",
       " 'TO',\n",
       " 'VBN',\n",
       " 'VBZ',\n",
       " 'PRP',\n",
       " 'VBG',\n",
       " 'VBP',\n",
       " 'MD',\n",
       " 'PRP$',\n",
       " 'POS',\n",
       " '$',\n",
       " '``',\n",
       " \"''\",\n",
       " ':',\n",
       " 'WDT',\n",
       " 'JJR',\n",
       " 'WP',\n",
       " 'WRB',\n",
       " 'NNPS',\n",
       " 'JJS',\n",
       " 'RBR',\n",
       " ')',\n",
       " '(',\n",
       " 'EX',\n",
       " 'RBS',\n",
       " 'RP',\n",
       " 'PDT',\n",
       " '#',\n",
       " 'FW',\n",
       " 'WP$',\n",
       " 'UH',\n",
       " 'SYM']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAG.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
