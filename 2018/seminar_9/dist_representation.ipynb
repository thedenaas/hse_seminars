{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed representations\n",
    "\n",
    "\n",
    "1. Sparse features  \n",
    "    1.1 Hashing trick  \n",
    "    1.2 Categorial features  \n",
    "    \n",
    "    Note about semi-supervised learning.\n",
    "    \n",
    "2. Word2vec  \n",
    "    2.1 skip-gram model  \n",
    "    2.2 continious bag of words model  \n",
    "    2.3 Co-occurence matrix  \n",
    "    2.4 Glove    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings\n",
    "1. (general 1) https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "1. (general 2) https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\n",
    "1. (glove ) https://towardsdatascience.com/emnlp-what-is-glove-part-i-3b6ce6a7f970\n",
    "1. (embeddings in pytorch) https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Sparse features\n",
    "### 1.1 Hashing trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically it is a substution (string_token) -> hash(string_token) of fixed size  \n",
    "    \n",
    "Hello, polynomial hash for strings and MurmurHash3 (used in sklearn)  \n",
    "\n",
    "Pros:\n",
    "    1. extrapolate on unseen words, scalable\n",
    "    2. reduce feature dimension\n",
    "Cons:\n",
    "    1. no inverse transform possible\n",
    "    2. collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate on US airlines twitter dataset for sentiment analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "SEED = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment         airline  retweet_count  \\\n",
       "0  570306133677760513           neutral  Virgin America              0   \n",
       "1  570301130888122368          positive  Virgin America              0   \n",
       "2  570301083672813571           neutral  Virgin America              0   \n",
       "3  570301031407624196          negative  Virgin America              0   \n",
       "4  570300817074462722          negative  Virgin America              0   \n",
       "\n",
       "                                                text  \n",
       "0                @VirginAmerica What @dhepburn said.  \n",
       "1  @VirginAmerica plus you've added commercials t...  \n",
       "2  @VirginAmerica I didn't today... Must mean I n...  \n",
       "3  @VirginAmerica it's really aggressive to blast...  \n",
       "4  @VirginAmerica and it's a really big bad thing...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train logloss 0.585451549361806\n",
      "test logloss 0.6163646385006908\n"
     ]
    }
   ],
   "source": [
    "y = LabelEncoder().fit_transform(df.airline_sentiment)\n",
    "\n",
    "df_train, df_test, y_train, y_test = model_selection.train_test_split(df, y, test_size=0.25, \n",
    "                                                                      stratify=y, # WHY\n",
    "                                                                      random_state=SEED, \n",
    "                                                                      shuffle=True) # WHY\n",
    "\n",
    "# model v1\n",
    "# Simple BOW model, binary matrix  \n",
    "# Let's try to reduce number of features with hashing\n",
    "model1 = Pipeline([\n",
    "    ('text_vect', HashingVectorizer(analyzer='word', n_features=500, ngram_range=(1,1), norm=None, binary=True)),\n",
    "    ('logreg', LogisticRegressionCV(Cs=10, cv=3, scoring='neg_log_loss', n_jobs=-1, \n",
    "                                    multi_class='multinomial', random_state=SEED))\n",
    "])\n",
    "\n",
    "model1.fit(df_train.text, y_train)\n",
    "print('train logloss', metrics.log_loss(y_train, model1.predict_proba(df_train.text)))\n",
    "print('test logloss', metrics.log_loss(y_test, model1.predict_proba(df_test.text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Categorial features in linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United            3822\n",
       "US Airways        2913\n",
       "American          2759\n",
       "Southwest         2420\n",
       "Delta             2222\n",
       "Virgin America     504\n",
       "Name: airline, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add categorial feature to our linear model with one-hot encoding\n",
    "\n",
    "# categorial features\n",
    "df.airline.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denaas/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot enc shape (10980, 6)\n",
      "train logloss 0.5845149676056038\n",
      "test logloss 0.6145427069493641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "text_vec = HashingVectorizer(analyzer='word', n_features=500, ngram_range=(1,1), norm=None, \n",
    "                             binary=True)\n",
    "X1_train = text_vec.fit_transform(df_train.text).toarray()\n",
    "\n",
    "tmp_le = LabelEncoder()\n",
    "X2_train = tmp_le.fit_transform(df_train.airline.values).reshape(-1,1)\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "X2_train = enc.fit_transform(X2_train)\n",
    "print('one-hot enc shape', X2_train.shape)\n",
    "\n",
    "X_train = np.hstack([X1_train, X2_train])\n",
    "\n",
    "model2 = LogisticRegressionCV(Cs=10, cv=3, scoring='neg_log_loss', n_jobs=-1, \n",
    "                                    multi_class='multinomial', random_state=SEED)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "X1_test = text_vec.transform(df_test.text).toarray()\n",
    "X2_test = tmp_le.transform(df_test.airline.values).reshape(-1,1)\n",
    "X2_test = enc.transform(X2_test)\n",
    "X_test = np.hstack([X1_test, X2_test])\n",
    "\n",
    "print('train logloss', metrics.log_loss(y_train, model2.predict_proba(X_train)))\n",
    "print('test logloss', metrics.log_loss(y_test, model2.predict_proba(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Word2vec\n",
    "![image](http://nlpx.net/wp/wp-content/uploads/2015/11/word2vec.png)\n",
    "### 2.1 Skip-gram model\n",
    "![image](https://i.stack.imgur.com/igSuE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word t predict surrounding words in a xindox of size m\n",
    "\n",
    "Objective is maximize probability of context words given the current center word:  \n",
    "    \n",
    "$$J(\\theta) = \\prod^T_{t=1} \\prod_{-m \\le j \\le m; j != 0 }  p(x_{t+j} | x_t; \\theta)  \\rightarrow max $$,\n",
    "xhere\n",
    "$x_t$ - center word,  \n",
    "$x_{t+j}$ - word from context,  \n",
    "$m$ - context size.  \n",
    "\n",
    "or negative log-likelihood:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum^T_{t=1} \\sum_{-m \\le j \\le m; j != 0 }  log p(x_{t+j} | x_t; \\theta)  \\rightarrow min $$\n",
    "\n",
    "$$p(x_{t+j} | x_t) = p(out | center) = \\frac{exp(u_{out}^T v_{center})}{\\sum_k=1^K exp(u_{k}^T v_{center})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Hierarchial Huffman trees\n",
    "\n",
    "Complexity $O(V) \\rightarrow O(\\log_2 V)$\n",
    "\n",
    "$x = v_{n(w,j)}^T v_{w}$,   \n",
    "where $n(w,j)$ is the j-th node on the path from the root to $w$.  \n",
    "\n",
    "$p(n, left) = \\sigma (v_n^T v_w)$ - probability to go to the left.  \n",
    "$p(n, right) = \\sigma (- v_n^T v_w )$ - probability to go to the right.  \n",
    "\n",
    "Then,  \n",
    "$p(w_j | w) = \\prod_{j=1}^{L(w) - 1} \\sigma ( [ n(w, j+1) == child(n(w,j)) ] v_n^T v_w)$,  \n",
    "where $L(w)$ - depth of the tree,  \n",
    "$child(n)$ - child of node n.\n",
    "\n",
    "\n",
    "<img src=\"images/hier.png\" style=\"height:300px\">\n",
    "\n",
    "How to build binary prefix tree? -> Huffman Tree.\n",
    "<img src=\"images/huffman.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling\n",
    "\n",
    "Using negative sampling with k samples:   \n",
    "    \n",
    "$log p(w_{t+j} | w_t; \\theta) = log \\sigma(u_{outer}^T v_{center})  + \\sum_{i=1}^k E_{j ~ P(w)} [log \\sigma (-u_j^T v_{center})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = df.text.apply(lambda x: x.split()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.73 s, sys: 13.4 ms, total: 3.75 s\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "w2v = Word2Vec(sentences, negative=5, size=100, iter=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('airline.', 0.9007248878479004),\n",
       " ('best', 0.8687483072280884),\n",
       " ('ever', 0.8615865111351013),\n",
       " ('awful', 0.8586821556091309),\n",
       " ('most', 0.8517616987228394),\n",
       " ('worst', 0.84912109375),\n",
       " ('disappointed', 0.8412606716156006),\n",
       " ('horrible', 0.838492751121521),\n",
       " ('company', 0.8372955322265625),\n",
       " ('absolute', 0.8371437788009644)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('airline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 CBOW model\n",
    "\n",
    "<img src=\"images/cbow.png\" style=\"height:500px\">\n",
    "\n",
    "$$h = W^T x$$  \n",
    "$$x = [x_{j-m}, x_{j-m+1}, ... x_{j-1}, x_{j+1}, ..., x_{j+m}] $$  \n",
    "\n",
    "$$p(x_j | x) = \\frac{exp(v_j^T h)}{\\sum_k=1^K exp(v_k^T h)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "w2v = Word2Vec(sentences, negative=5, size=100, iter=100, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('assult', 0.6154369115829468),\n",
       " ('reported', 0.45402228832244873),\n",
       " ('most', 0.4082372188568115),\n",
       " ('communication,', 0.40658941864967346),\n",
       " ('Delays', 0.38628947734832764),\n",
       " ('Gate', 0.38524293899536133),\n",
       " ('Atlantic', 0.38465529680252075),\n",
       " ('engine', 0.3818504810333252),\n",
       " ('computer', 0.3697021007537842),\n",
       " ('SNA', 0.35934001207351685)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('police')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Co-occurence matrix\n",
    "\n",
    "<img src=\"images/matrix.png\" style=\"height:300px\">\n",
    "\n",
    "$P_{ij}$ - occurance of i-th word along with j-th in the window of size m\n",
    "\n",
    "Cons: \n",
    "1. Very high-dimensional, not used in practice\n",
    "2. Hard to add new words and docs\n",
    "\n",
    "Trivial solution: use some dimension-reduction method, usually SVD\n",
    "\n",
    "Singular Value Decomposition\n",
    "\n",
    "$M = U \\Sigma V$  \n",
    "$Mv = \\sigma u$  \n",
    "$M^{*}u = \\sigma v$   \n",
    "U, V are unitary matrices  \n",
    "$\\Sigma$ - diagonal\n",
    "\n",
    "\n",
    "$O(nm^2)$ for case n < m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Glove\n",
    "\n",
    "<img src=\"images/glove.png\" style=\"height:300px\">\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2} \\sum_{i,j=1}^W f(P_{ij})(u_i^T v_j - log P_{ij})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
